{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AditiJHA-ai/Hackerix/blob/main/bajaj_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "ADNtQgxAAplm",
        "outputId": "c55f2b3a-080e-4388-91fa-8c842995b3d7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'PDFQuery_LangChain.ipynb\\nAutomatically generated by Colab.\\nOriginal file is located at\\n    https://colab.research.google.com/drive/1XMyeWxw9gnhCZfjP22TOfNB0uDfdjZ99\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"PDFQuery_LangChain.ipynb\n",
        "Automatically generated by Colab.\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1XMyeWxw9gnhCZfjP22TOfNB0uDfdjZ99\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RYKc2OsApfo",
        "outputId": "cc42c586-ec95-4178-aaa3-b6287b1cf5d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.11/dist-packages (4.14.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.99.1)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.9.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (5.0.0)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from pymongo) (2.7.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.72)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.12)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.42)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.55.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.16.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.34.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.7)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: unstructured in /usr/local/lib/python3.11/dist-packages (0.18.11)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.2.0)\n",
            "Requirement already satisfied: docx2txt in /usr/local/lib/python3.11/dist-packages (0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (1.17.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.72)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.12)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.42)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from unstructured) (3.4.2)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.11/dist-packages (from unstructured) (1.2.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.11/dist-packages (from unstructured) (0.4.27)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from unstructured) (5.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from unstructured) (3.9.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from unstructured) (4.13.4)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.11/dist-packages (from unstructured) (2.14.1)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from unstructured) (0.6.7)\n",
            "Requirement already satisfied: python-iso639 in /usr/local/lib/python3.11/dist-packages (from unstructured) (2025.2.18)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (from unstructured) (1.0.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unstructured) (2.0.2)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.11/dist-packages (from unstructured) (3.13.0)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.11/dist-packages (from unstructured) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from unstructured) (4.14.1)\n",
            "Requirement already satisfied: unstructured-client in /usr/local/lib/python3.11/dist-packages (from unstructured) (0.42.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from unstructured) (1.17.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unstructured) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unstructured) (5.9.5)\n",
            "Requirement already satisfied: python-oxmsg in /usr/local/lib/python3.11/dist-packages (from unstructured) (0.0.2)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.11/dist-packages (from unstructured) (1.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->unstructured) (2.7)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->unstructured) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->unstructured) (0.9.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from html5lib->unstructured) (0.5.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured) (2024.11.6)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.11/dist-packages (from python-oxmsg->unstructured) (0.47)\n",
            "Requirement already satisfied: aiofiles>=24.1.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (24.1.0)\n",
            "Requirement already satisfied: cryptography>=3.1 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (43.0.3)\n",
            "Requirement already satisfied: httpcore>=1.0.9 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (1.0.9)\n",
            "Requirement already satisfied: pypdf>=4.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (5.9.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=3.1->unstructured-client->unstructured) (1.17.1)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore>=1.0.9->unstructured-client->unstructured) (0.16.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.10.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured) (1.1.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client->unstructured) (2.22)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.72)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.42)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.12)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.9)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (2.33.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0.post1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: langchain-huggingface in /usr/local/lib/python3.11/dist-packages (0.3.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.70 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.3.72)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.21.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.33.4 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.34.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (1.1.7)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.4.12)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.33)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (2.11.7)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.33.4->langchain-huggingface) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.33.4->langchain-huggingface) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.33.4->langchain-huggingface) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.33.4->langchain-huggingface) (2025.8.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "# ‚úÖ FIX pip typo\n",
        "!pip install pymongo openai langchain pypdf sentence-transformers\n",
        "!pip install langchain unstructured python-docx docx2txt six\n",
        "!pip install -U langchain-community\n",
        "!pip install faiss-cpu\n",
        "!pip install -U langchain-huggingface\n",
        "from pymongo import MongoClient\n",
        "!pip install streamlit pyngrok --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiTWDdmzApXy"
      },
      "outputs": [],
      "source": [
        "# MongoDB connection\n",
        "mongo_uri = \"mongodb+srv://anubhutianurag216:d7iADMpcN6ZvfCeo@cluster0.mn6dkjp.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
        "client = MongoClient(mongo_uri)\n",
        "\n",
        "# Define database and collection\n",
        "db = client[\"policy_ai\"]\n",
        "collection = db[\"clauses\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQdGrTweApP_"
      },
      "outputs": [],
      "source": [
        "# üìÑ Document loaders\n",
        "from pathlib import Path\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.document_loaders import (\n",
        "    PyPDFLoader,\n",
        "    UnstructuredWordDocumentLoader,\n",
        "    UnstructuredEmailLoader,\n",
        ")\n",
        "from email import policy\n",
        "from email.parser import BytesParser\n",
        "\n",
        "def load_documents(path: str) -> list[Document]:\n",
        "    ext = Path(path).suffix.lower()\n",
        "\n",
        "    if ext == \".pdf\":\n",
        "        loader = PyPDFLoader(path)\n",
        "        docs = loader.load()\n",
        "    elif ext == \".docx\":\n",
        "        loader = UnstructuredWordDocumentLoader(path)\n",
        "        docs = loader.load()\n",
        "    elif ext in [\".eml\", \".msg\"]:\n",
        "        try:\n",
        "            loader = UnstructuredEmailLoader(path)\n",
        "            docs = loader.load()\n",
        "        except Exception:\n",
        "            with open(path, \"rb\") as f:\n",
        "                msg = BytesParser(policy=policy.default).parse(f)\n",
        "            body = msg.get_body(preferencelist=(\"plain\",)).get_content()\n",
        "            docs = [Document(page_content=body, metadata={\"source\": path})]\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported file type: {ext}\")\n",
        "\n",
        "    docs = [doc if isinstance(doc, Document) else Document(**doc) for doc in docs]\n",
        "    return docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GYg3lrlcBH7i",
        "outputId": "60f3b322-ad26-4109-d59a-e36b5be1e759"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ae0f7a16-1a85-4445-851f-e9455fea1f8f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ae0f7a16-1a85-4445-851f-e9455fea1f8f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving BAJHLIP23020V012223.pdf to BAJHLIP23020V012223 (1).pdf\n",
            "‚úÖ Loaded 49 document chunks from BAJHLIP23020V012223 (1).pdf\n",
            "\n",
            "--- Chunk 1 ---\n",
            "UIN- BAJHLIP23020V012223                                 Global Health Care/ Policy Wordings/Page 1 \n",
            " \n",
            " \n",
            "Bajaj Allianz General Insurance Co. Ltd.                       \n",
            "Bajaj Allianz House, Airport Road, Yerawada, Pune - 411 006. Reg. No.: 113 \n",
            "For more details, log on to: www.bajajallianz.com | E-mail: bagichelp@bajajallianz.co.in or \n",
            "Call at: Sales - 1800 209 0144 / Service - 1800 209 5858 (Toll Free No.) \n",
            "Issuing Office: \n",
            " \n",
            "GLOBAL HEALTH CARE \n",
            " \n",
            " \n",
            "Policy Wordings \n",
            " \n",
            "UIN- BAJHLIP23020V012223\n",
            "\n",
            "--- Chunk 2 ---\n",
            "Policy Wordings \n",
            " \n",
            "UIN- BAJHLIP23020V012223 \n",
            "SECTION A) PREAMBLE \n",
            " \n",
            "Whereas the Insured described in the Policy Schedule hereto (hereinafter called the ‚ÄòInsured‚Äô  or ‚ÄúPolicyholder‚Äù or \n",
            "‚ÄúInsured Person‚Äù) has made to Bajaj Allianz General Insurance Company Limited (hereinafter called the ‚ÄúCompany‚Äù \n",
            "or ‚ÄúInsurer‚Äù or ‚ÄúInsurance Company‚Äù) a proposal or Proposal as mentioned in the transcript of the Proposal, which\n",
            "\n",
            "--- Chunk 3 ---\n",
            "shall be the basis of this Contract and is deemed to be incorporated herein, containing certain undertakings , \n",
            "declarations, information/particulars and statements, which is hereby agreed to be the basis of this Contract and be \n",
            "considered as incorporated herein, for the insurance Contract hereinafter contained and has paid the premium \n",
            "specified in the Policy Sche dule hereto as consideration for such insurance Contract, now the Company agrees,\n",
            "\n",
            "--- Chunk 4 ---\n",
            "subject always to the Policy Schedule and the following terms, conditions, exclusions, and limitations of the Policy, \n",
            "and in excess of the amount of the Deductible/ Co-Payment, to indemnify the Insured in respect of an admissible \n",
            "claim in the manner and to the extent hereinafter stated. \n",
            " \n",
            "SECTION B) DEFINITIONS - STANDARD DEFINITIONS \n",
            "Words or terms mentioned below have the meaning ascribed to them wherever they appear in this Po licy, and\n",
            "\n",
            "--- Chunk 5 ---\n",
            "references to the singular or to the masculine, include references to the plural or to the feminine wherever the context \n",
            "permits. If any word starts with Capital alphabet but is not defined in the Standard Definitions or Specific Definitions, \n",
            "then such word shall be interpreted as per the headings of the respective clauses/points in these Policy Wordings. \n",
            " \n",
            "1. Accident:- \n",
            "An Accident means sudden, unforeseen and involuntary event caused by external, visible and violent means.\n",
            "‚úÖ FAISS vector store updated with improved chunking.\n"
          ]
        }
      ],
      "source": [
        "# ‚úÖ Upload & reprocess with better chunking\n",
        "from google.colab import files\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings  # ‚úÖ Add this line\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "uploaded = files.upload()\n",
        "file_path = list(uploaded.keys())[0]\n",
        "\n",
        "documents = load_documents(file_path)\n",
        "print(f\"‚úÖ Loaded {len(documents)} document chunks from {file_path}\")\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=50,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
        ")\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "for i, chunk in enumerate(texts[:5]):\n",
        "    print(f\"\\n--- Chunk {i+1} ---\\n{chunk.page_content}\")\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "db = FAISS.from_documents(texts, embedding_model)\n",
        "db.save_local(\"faiss_index\")\n",
        "\n",
        "print(\"‚úÖ FAISS vector store updated with improved chunking.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdjpCn_GEnB8",
        "outputId": "ff79427d-5449-4d19-c8b9-f92589cee847"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.11/dist-packages (2.1.9)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (1.2.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage<0.7.0,>=0.6.18 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.6.18)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.68 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.3.72)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (2.11.7)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.29.5)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (0.4.12)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.74.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (0.23.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (0.16.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        " !pip install -U langchain-google-genai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKl_079-BhWY",
        "outputId": "47cdfae6-df10-48dd-c13c-9129b3cb8bb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Gemini QA pipeline is ready. You can now ask questions.\n"
          ]
        }
      ],
      "source": [
        "# ‚úÖ Gemini LLM QA Setup\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "db = FAISS.load_local(\"faiss_index\", embedding_model, allow_dangerous_deserialization=True)\n",
        "retriever = db.as_retriever()\n",
        "\n",
        "GOOGLE_API_KEY = \"AIzaSyBJFW2KdeAcIDH3vUOQJUY682Rr0v7s2QY\"  # Replace with actual key\n",
        "GEMINI_MODEL_NAME = \"gemini-1.5-flash\"  # <-- Update this as needed\n",
        "gemini_llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", google_api_key=GOOGLE_API_KEY)\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "You are an expert assistant for insurance and policy document analysis. Given the following context (extracted clauses) and a user query, answer ONLY using the information in the context.\n",
        "\n",
        "Return your answer as a JSON object with the following fields:\n",
        "  decision: (string, e.g., 'approved', 'rejected', 'covered', 'not covered', etc.)\n",
        "  amount: (number or null, payout amount if applicable)\n",
        "  justification: (string, concise explanation for the decision)\n",
        "  clause_mapping: (list of objects, each with 'clause_text' and 'source' fields, mapping the decision to the specific clause(s) used)\n",
        "\n",
        "Example output:\n",
        "{\n",
        "  \"decision\": \"approved\",\n",
        "  \"amount\": 50000,\n",
        "  \"justification\": \"Knee surgery is covered for policies older than 3 months as per clause 4.2.\",\n",
        "  \"clause_mapping\": [\n",
        "    {\"clause_text\": \"Knee surgery is covered after 3 months of policy inception.\", \"source\": \"policy.pdf\"}\n",
        "  ]\n",
        "}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=gemini_llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": prompt_template}\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Gemini QA pipeline is ready. You can now ask questions.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZwv1YGEF5GT",
        "outputId": "47454672-00d6-4d2e-8870-7dd04e9ea0a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç Raw Gemini Output:\n",
            "```json\n",
            "{\n",
            "  \"decision\": \"covered after waiting period\",\n",
            "  \"amount\": null,\n",
            "  \"justification\": \"The waiting period for pre-existing diseases is 36 months of continuous coverage after the policy's inception date.  Expenses related to treatment of a pre-existing disease and its direct complications are excluded until this waiting period expires.\",\n",
            "  \"clause_mapping\": [\n",
            "    {\n",
            "      \"clause_text\": \"Expenses related to the treatment of a Pre-Existing Disease (PED) and its direct complications shall be excluded until the expiry of 36 months of continuous coverage after the date of inception of the first Global Health Care Policy with Us.\",\n",
            "      \"source\": \"Clause 37 & 38f1a\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "```\n",
            "\n",
            "üîç Structured JSON Answer:\n",
            "{\n",
            "  \"decision\": \"covered after waiting period\",\n",
            "  \"amount\": null,\n",
            "  \"justification\": \"The waiting period for pre-existing diseases is 36 months of continuous coverage after the policy's inception date.  Expenses related to treatment of a pre-existing disease and its direct complications are excluded until this waiting period expires.\",\n",
            "  \"clause_mapping\": [\n",
            "    {\n",
            "      \"clause_text\": \"Expenses related to the treatment of a Pre-Existing Disease (PED) and its direct complications shall be excluded until the expiry of 36 months of continuous coverage after the date of inception of the first Global Health Care Policy with Us.\",\n",
            "      \"source\": \"Clause 37 & 38f1a\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            "üìÑ Source Documents:\n",
            "- BAJHLIP23020V012223 (1).pdf\n",
            "- BAJHLIP23020V012223 (1).pdf\n",
            "- BAJHLIP23020V012223 (1).pdf\n",
            "- BAJHLIP23020V012223 (1).pdf\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains.combine_documents.stuff import create_stuff_documents_chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "import json\n",
        "import re\n",
        "\n",
        "# 1. Define the structured prompt\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=\"\"\"\n",
        "You are an expert assistant for insurance and policy document analysis. Given the following context (extracted clauses) and a user query, answer ONLY using the information in the context.\n",
        "\n",
        "Return your answer as a JSON object with the following fields:\n",
        "  decision: (string, e.g., 'approved', 'rejected', 'covered', 'not covered', etc.)\n",
        "  amount: (number or null, payout amount if applicable)\n",
        "  justification: (string, concise explanation for the decision)\n",
        "  clause_mapping: (list of objects, each with 'clause_text' and 'source' fields, mapping the decision to the specific clause(s) used)\n",
        "\n",
        "Example output:\n",
        "{{\n",
        "  \"decision\": \"approved\",\n",
        "  \"amount\": 50000,\n",
        "  \"justification\": \"Knee surgery is covered for policies older than 3 months as per clause 4.2.\",\n",
        "  \"clause_mapping\": [\n",
        "    {{\"clause_text\": \"Knee surgery is covered after 3 months of policy inception.\", \"source\": \"policy.pdf\"}}\n",
        "  ]\n",
        "}}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        " )\n",
        "\n",
        "# 2. Create the document QA chain\n",
        "stuff_chain = create_stuff_documents_chain(llm=gemini_llm, prompt=prompt_template)\n",
        "\n",
        "# 3. Query\n",
        "query = \"What is the waiting period for pre-existing diseases?\"\n",
        "\n",
        "# 4. Retrieve relevant documents\n",
        "docs = retriever.invoke(query)\n",
        "\n",
        "# 5. Run the chain\n",
        "result = stuff_chain.invoke({\n",
        "    \"context\": docs,\n",
        "    \"question\": query\n",
        "})\n",
        "\n",
        "# 6. Display the output\n",
        "print(\"\\nüîç Raw Gemini Output:\")\n",
        "print(result)\n",
        "\n",
        "print(\"\\nüîç Structured JSON Answer:\")\n",
        "try:\n",
        "    # Clean up result if it contains markdown or code block\n",
        "    if isinstance(result, str):\n",
        "        cleaned = re.sub(r\"^```json|^```|```$\", \"\", result.strip(), flags=re.MULTILINE).strip()\n",
        "        answer_json = json.loads(cleaned)\n",
        "    else:\n",
        "        answer_json = result\n",
        "    print(json.dumps(answer_json, indent=2))\n",
        "except Exception as e:\n",
        "    print(\"‚ùå Could not parse result as JSON. Raw output:\")\n",
        "    print(result)\n",
        "    print(\"Error:\", e)\n",
        "\n",
        "print(\"\\nüìÑ Source Documents:\")\n",
        "try:\n",
        "    for doc in docs:\n",
        "        print(f\"- {getattr(doc, 'metadata', {}).get('source', 'unknown')}\")\n",
        "except Exception as e:\n",
        "    print(\"‚ùå Error printing source documents. docs object:\", docs)\n",
        "    print(\"Error:\", e)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to process multiple queries\n",
        "def process_multiple_queries(queries):\n",
        "    print(\"Processing multiple queries...\\n\")\n",
        "    results = {}\n",
        "\n",
        "    for i, query in enumerate(queries, 1):\n",
        "        print(f\"\\nüìù Query {i}: {query}\")\n",
        "        try:\n",
        "            # Retrieve relevant documents\n",
        "            docs = retriever.invoke(query)\n",
        "\n",
        "            # Run the chain\n",
        "            result = stuff_chain.invoke({\n",
        "                \"context\": docs,\n",
        "                \"question\": query\n",
        "            })\n",
        "\n",
        "            # Process the result\n",
        "            if isinstance(result, str):\n",
        "                cleaned = re.sub(r\"^```json|^```|```$\", \"\", result.strip(), flags=re.MULTILINE).strip()\n",
        "                answer_json = json.loads(cleaned)\n",
        "            else:\n",
        "                answer_json = result\n",
        "\n",
        "            results[query] = {\n",
        "                \"answer\": answer_json,\n",
        "                \"sources\": [getattr(doc, 'metadata', {}).get('source', 'unknown') for doc in docs]\n",
        "            }\n",
        "\n",
        "            print(\"‚úÖ Query processed successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            results[query] = {\n",
        "                \"error\": str(e)\n",
        "            }\n",
        "            print(f\"‚ùå Error processing query: {str(e)}\")\n",
        "            if \"429\" in str(e):\n",
        "                print(\"Rate limit reached. Waiting 60 seconds before next query...\")\n",
        "                time.sleep(60)\n",
        "\n",
        "    return results\n",
        "\n",
        "# Example usage with multiple queries\n",
        "queries = [\n",
        "    \"What is the waiting period for pre-existing diseases?\",\n",
        "    \"What are the exclusions in this policy?\",\n",
        "    \"What is the coverage amount for hospitalization?\"\n",
        "]\n",
        "\n",
        "# Process all queries\n",
        "results = process_multiple_queries(queries)\n",
        "\n",
        "# Display results\n",
        "print(\"\\nüîç Results for all queries:\")\n",
        "for query, result in results.items():\n",
        "    print(f\"\\nQuery: {query}\")\n",
        "    if \"error\" in result:\n",
        "        print(f\"‚ùå Error: {result['error']}\")\n",
        "    else:\n",
        "        print(\"Answer:\")\n",
        "        print(json.dumps(result['answer'], indent=2))\n",
        "        print(\"\\nSources:\", result['sources'])"
      ],
      "metadata": {
        "id": "mWrS40PZLcQC",
        "outputId": "fdb8ac68-3cfa-487d-bac4-7abb98f877d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing multiple queries...\n",
            "\n",
            "\n",
            "üìù Query 1: What is the waiting period for pre-existing diseases?\n",
            "‚úÖ Query processed successfully\n",
            "\n",
            "üìù Query 2: What are the exclusions in this policy?\n",
            "‚úÖ Query processed successfully\n",
            "\n",
            "üìù Query 3: What is the coverage amount for hospitalization?\n",
            "‚úÖ Query processed successfully\n",
            "\n",
            "üîç Results for all queries:\n",
            "\n",
            "Query: What is the waiting period for pre-existing diseases?\n",
            "Answer:\n",
            "{\n",
            "  \"decision\": \"covered after waiting period\",\n",
            "  \"amount\": null,\n",
            "  \"justification\": \"The waiting period for pre-existing diseases is 36 months of continuous coverage after the policy inception date.  Expenses related to treatment of pre-existing diseases and their direct complications are excluded until this waiting period expires.\",\n",
            "  \"clause_mapping\": [\n",
            "    {\n",
            "      \"clause_text\": \"Expenses related to the treatment of a Pre-Existing Disease (PED) and its direct complications shall be excluded until the expiry of 36 months of continuous coverage after the date of inception of the first Global Health Care Policy with Us.\",\n",
            "      \"source\": \"Clause 37 & 38f1a\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            "Sources: ['BAJHLIP23020V012223 (1).pdf', 'BAJHLIP23020V012223 (1).pdf', 'BAJHLIP23020V012223 (1).pdf', 'BAJHLIP23020V012223 (1).pdf']\n",
            "\n",
            "Query: What are the exclusions in this policy?\n",
            "Answer:\n",
            "{\n",
            "  \"decision\": \"not covered\",\n",
            "  \"amount\": null,\n",
            "  \"justification\": \"The policy excludes dental treatment that includes cosmetic surgery, dentures, dental prosthesis, and dental implants, unless otherwise specified in the Table of Benefits or a written policy endorsement.\",\n",
            "  \"clause_mapping\": [\n",
            "    {\n",
            "      \"clause_text\": \"We do not cover the following expenses unless indicated otherwise in the Table of Benefits or in any written Policy endorsement\\n\\n1) Any  Dental Treatment  that comprises of cosmetic surgery, dentures, dental prosthesis, dental implants,\",\n",
            "      \"source\": \"Section D) EXCLUSIONS\\u2013 SPECIFIC EXCLUSIONS APPLICABLE TO PART A- DOMESTIC COVER UNDER SECTION C) BENEFITS COVERED UNDER THE POLICY\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            "Sources: ['BAJHLIP23020V012223 (1).pdf', 'BAJHLIP23020V012223 (1).pdf', 'BAJHLIP23020V012223 (1).pdf', 'BAJHLIP23020V012223 (1).pdf']\n",
            "\n",
            "Query: What is the coverage amount for hospitalization?\n",
            "Answer:\n",
            "{\n",
            "  \"decision\": \"covered\",\n",
            "  \"amount\": null,\n",
            "  \"justification\": \"The provided text specifies coverage for in-patient hospitalization treatment, but the exact amount depends on the plan and sum insured.  The Imperial Plan offers INR 3,750,000, INR 5,600,000, INR 7,500,000, INR 11,200,000, INR 18,750,000, and INR 37,500,000 while the Imperial Plus Plan offers different amounts.  More information is needed to determine a specific amount.\",\n",
            "  \"clause_mapping\": [\n",
            "    {\n",
            "      \"clause_text\": \"In-patient Hospitalization Treatment Limits\\nINR 3,750,000   INR 5,600,000  INR 7,500,000  INR 11,200,000  INR 18,750,000  INR 37,500,000\",\n",
            "      \"source\": \"TABLE OF BENEFITS FOR DOMESTIC COVER\"\n",
            "    },\n",
            "    {\n",
            "      \"clause_text\": \"In-patient Hospitalization Treatment Up to Sum Insured\",\n",
            "      \"source\": \"TABLE OF BENEFITS FOR DOMESTIC COVER\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            "Sources: ['BAJHLIP23020V012223 (1).pdf', 'BAJHLIP23020V012223 (1).pdf', 'BAJHLIP23020V012223 (1).pdf', 'BAJHLIP23020V012223 (1).pdf']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WpjBol4QE71"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "genai.configure(api_key=\"AIzaSyBJFW2KdeAcIDH3vUOQJUY682Rr0v7s2QY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qf7OMs1VQzpb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "94e38149-a3fa-4aaa-c968-dc46edf1416f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/embedding-gecko-001\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-1.5-pro-002\n",
            "models/gemini-1.5-pro\n",
            "models/gemini-1.5-flash-latest\n",
            "models/gemini-1.5-flash\n",
            "models/gemini-1.5-flash-002\n",
            "models/gemini-1.5-flash-8b\n",
            "models/gemini-1.5-flash-8b-001\n",
            "models/gemini-1.5-flash-8b-latest\n",
            "models/gemini-2.5-pro-preview-03-25\n",
            "models/gemini-2.5-flash-preview-05-20\n",
            "models/gemini-2.5-flash\n",
            "models/gemini-2.5-flash-lite-preview-06-17\n",
            "models/gemini-2.5-pro-preview-05-06\n",
            "models/gemini-2.5-pro-preview-06-05\n",
            "models/gemini-2.5-pro\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-exp-image-generation\n",
            "models/gemini-2.0-flash-lite-001\n",
            "models/gemini-2.0-flash-lite\n",
            "models/gemini-2.0-flash-preview-image-generation\n",
            "models/gemini-2.0-flash-lite-preview-02-05\n",
            "models/gemini-2.0-flash-lite-preview\n",
            "models/gemini-2.0-pro-exp\n",
            "models/gemini-2.0-pro-exp-02-05\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.0-flash-thinking-exp-01-21\n",
            "models/gemini-2.0-flash-thinking-exp\n",
            "models/gemini-2.0-flash-thinking-exp-1219\n",
            "models/gemini-2.5-flash-preview-tts\n",
            "models/gemini-2.5-pro-preview-tts\n",
            "models/learnlm-2.0-flash-experimental\n",
            "models/gemma-3-1b-it\n",
            "models/gemma-3-4b-it\n",
            "models/gemma-3-12b-it\n",
            "models/gemma-3-27b-it\n",
            "models/gemma-3n-e4b-it\n",
            "models/gemma-3n-e2b-it\n",
            "models/gemini-2.5-flash-lite\n",
            "models/embedding-001\n",
            "models/text-embedding-004\n",
            "models/gemini-embedding-exp-03-07\n",
            "models/gemini-embedding-exp\n",
            "models/gemini-embedding-001\n",
            "models/aqa\n",
            "models/imagen-3.0-generate-002\n",
            "models/imagen-4.0-generate-preview-06-06\n",
            "models/imagen-4.0-ultra-generate-preview-06-06\n",
            "models/veo-2.0-generate-001\n",
            "models/veo-3.0-generate-preview\n",
            "models/veo-3.0-fast-generate-preview\n",
            "models/gemini-2.5-flash-preview-native-audio-dialog\n",
            "models/gemini-2.5-flash-exp-native-audio-thinking-dialog\n",
            "models/gemini-2.0-flash-live-001\n",
            "models/gemini-live-2.5-flash-preview\n",
            "models/gemini-2.5-flash-live-preview\n"
          ]
        }
      ],
      "source": [
        "models = list(genai.list_models())\n",
        "for m in models:\n",
        "    print(m.name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.document_loaders import (\n",
        "    PyPDFLoader,\n",
        "    UnstructuredWordDocumentLoader,\n",
        "    UnstructuredEmailLoader,\n",
        ")\n",
        "from email import policy\n",
        "from email.parser import BytesParser\n",
        "\n",
        "\n",
        "# ----------------- Document Loader -----------------\n",
        "def load_documents(path: str) -> list[Document]:\n",
        "    ext = Path(path).suffix.lower()\n",
        "\n",
        "    if ext == \".pdf\":\n",
        "        loader = PyPDFLoader(path)\n",
        "        docs = loader.load()\n",
        "    elif ext == \".docx\":\n",
        "        loader = UnstructuredWordDocumentLoader(path)\n",
        "        docs = loader.load()\n",
        "    elif ext in [\".eml\", \".msg\"]:\n",
        "        try:\n",
        "            loader = UnstructuredEmailLoader(path)\n",
        "            docs = loader.load()\n",
        "        except Exception:\n",
        "            with open(path, \"rb\") as f:\n",
        "                msg = BytesParser(policy=policy.default).parse(f)\n",
        "            body = msg.get_body(preferencelist=(\"plain\",)).get_content()\n",
        "            docs = [Document(page_content=body, metadata={\"source\": path})]\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported file type: {ext}\")\n",
        "\n",
        "    docs = [doc if isinstance(doc, Document) else Document(**doc) for doc in docs]\n",
        "    return docs\n",
        "\n",
        "\n",
        "# ----------------- File Downloader -----------------\n",
        "def download_file(url: str, save_path: str) -> str:\n",
        "    r = requests.get(url)\n",
        "    r.raise_for_status()\n",
        "    with open(save_path, \"wb\") as f:\n",
        "        f.write(r.content)\n",
        "    return save_path\n",
        "\n",
        "\n",
        "# ----------------- Main Process -----------------\n",
        "def process_query(document_url, questions):\n",
        "    # 1Ô∏è‚É£ Download file\n",
        "    ext = Path(document_url).suffix.lower()\n",
        "    local_path = f\"/tmp/temp{ext}\"\n",
        "    download_file(document_url, local_path)\n",
        "\n",
        "    # 2Ô∏è‚É£ Load document\n",
        "    docs = load_documents(local_path)\n",
        "    if not docs:\n",
        "        raise ValueError(\"No text found in the document. Check if the file is valid.\")\n",
        "\n",
        "    # 3Ô∏è‚É£ Chunk the document\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=500,\n",
        "        chunk_overlap=50,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
        "    )\n",
        "    chunks = splitter.split_documents(docs)\n",
        "\n",
        "    # 4Ô∏è‚É£ Embed & store\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "    vectorstore = FAISS.from_documents(chunks, embeddings)\n",
        "    retriever = vectorstore.as_retriever()\n",
        "\n",
        "    # 5Ô∏è‚É£ LLM Setup\n",
        "    llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.1)\n",
        "\n",
        "    # 6Ô∏è‚É£ Custom Prompt\n",
        "    prompt_template = PromptTemplate.from_template(\n",
        "        \"You are an expert assistant. Based on the following document context:\\n\\n{context}\\n\\nAnswer the question:\\n{question}\"\n",
        "    )\n",
        "\n",
        "    # 7Ô∏è‚É£ QA Chain\n",
        "    qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        retriever=retriever,\n",
        "        chain_type=\"stuff\",\n",
        "        return_source_documents=False,\n",
        "        chain_type_kwargs={\"prompt\": prompt_template}\n",
        "    )\n",
        "\n",
        "    # 8Ô∏è‚É£ Answer questions\n",
        "    answers = []\n",
        "    for q in questions:\n",
        "        try:\n",
        "            answers.append(qa_chain.run(q))\n",
        "        except Exception as e:\n",
        "            answers.append(f\"Error processing question: {q} ‚Äî {str(e)}\")\n",
        "\n",
        "    return answers\n"
      ],
      "metadata": {
        "id": "dwFsFkIus3nq"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "\n",
        "# Paste your process_query function here if you want it self-contained\n",
        "import streamlit as st\n",
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from pyngrok import ngrok\n",
        "import os\n",
        "\n",
        "# ---- Paste the function here instead of importing it ----\n",
        "def process_query(document_url, questions):\n",
        "    loader = UnstructuredURLLoader(urls=[document_url])\n",
        "    docs = loader.load()\n",
        "\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=500,\n",
        "        chunk_overlap=50,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
        "    )\n",
        "    chunks = splitter.split_documents(docs)\n",
        "\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "    vectorstore = FAISS.from_documents(chunks, embeddings)\n",
        "    retriever = vectorstore.as_retriever()\n",
        "\n",
        "    llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.1)\n",
        "    prompt_template = PromptTemplate.from_template(\n",
        "        \"You are an expert assistant. Based on the following document context:\\n\\n{context}\\n\\nAnswer the question:\\n{question}\"\n",
        "    )\n",
        "\n",
        "    qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        retriever=retriever,\n",
        "        chain_type=\"stuff\",\n",
        "        return_source_documents=False,\n",
        "        chain_type_kwargs={\"prompt\": prompt_template}\n",
        "    )\n",
        "\n",
        "    answers = []\n",
        "    for q in questions:\n",
        "        try:\n",
        "            answers.append(qa_chain.run(q))\n",
        "        except Exception as e:\n",
        "            answers.append(f\"Error processing question: {q} ‚Äî {str(e)}\")\n",
        "\n",
        "    return answers\n",
        "\n",
        "# ---- Your Streamlit app code starts here ----\n",
        "st.title(\"Document Q&A\")\n",
        "  # Only if in another file, else paste the function above\n",
        "\n",
        "st.title(\"üìÑ Bajaj Policy Q&A\")\n",
        "document_url = st.text_input(\"Enter document URL\")\n",
        "questions_input = st.text_area(\"Enter questions (one per line)\")\n",
        "\n",
        "if st.button(\"Get Answers\"):\n",
        "    if document_url and questions_input.strip():\n",
        "        questions = [q.strip() for q in questions_input.split(\"\\n\") if q.strip()]\n",
        "        answers = process_query(document_url, questions)\n",
        "        for q, a in zip(questions, answers):\n",
        "            st.markdown(f\"**Q:** {q}\\n\\n**A:** {a}\")\n",
        "    else:\n",
        "        st.warning(\"Please enter both a document URL and at least one question.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EejsCiWI_cso",
        "outputId": "2514df86-1caf-44ad-8e16-f7ea29a99340"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pyngrok --quiet\n",
        "!pkill -f ngrok  # Kill previous tunnels\n",
        "\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Set your ngrok token\n",
        "ngrok.set_auth_token(\"313c7einCCU1hVcheudaUHmMTDF_2cYUiCfXeae7fZCVvjKoJ\")\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"‚úÖ Public Streamlit URL:\", public_url)\n",
        "\n",
        "# Run Streamlit app in background\n",
        "!streamlit run app.py & sleep 5\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yfkAt0x_gNY",
        "outputId": "8878daaa-76d1-409b-e6a0-e277178e696f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Public Streamlit URL: NgrokTunnel: \"https://3d20280223e8.ngrok-free.app\" -> \"http://localhost:8501\"\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.185.137.38:8501\u001b[0m\n",
            "\u001b[0m\n",
            "/content/app.py:5: LangChainDeprecationWarning: Importing UnstructuredURLLoader from langchain.document_loaders is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.document_loaders import UnstructuredURLLoader\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.document_loaders import UnstructuredURLLoader\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.document_loaders import UnstructuredURLLoader\n",
            "/content/app.py:7: LangChainDeprecationWarning: Importing HuggingFaceEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.embeddings import HuggingFaceEmbeddings\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.embeddings import HuggingFaceEmbeddings\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.embeddings import HuggingFaceEmbeddings\n",
            "/content/app.py:8: LangChainDeprecationWarning: Importing FAISS from langchain.vectorstores is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.vectorstores import FAISS\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.vectorstores import FAISS\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.vectorstores import FAISS\n",
            "/content/app.py:5: LangChainDeprecationWarning: Importing UnstructuredURLLoader from langchain.document_loaders is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.document_loaders import UnstructuredURLLoader\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.document_loaders import UnstructuredURLLoader\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.document_loaders import UnstructuredURLLoader\n",
            "/content/app.py:7: LangChainDeprecationWarning: Importing HuggingFaceEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.embeddings import HuggingFaceEmbeddings\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.embeddings import HuggingFaceEmbeddings\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.embeddings import HuggingFaceEmbeddings\n",
            "/content/app.py:8: LangChainDeprecationWarning: Importing FAISS from langchain.vectorstores is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.vectorstores import FAISS\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.vectorstores import FAISS\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.vectorstores import FAISS\n",
            "/content/app.py:5: LangChainDeprecationWarning: Importing UnstructuredURLLoader from langchain.document_loaders is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.document_loaders import UnstructuredURLLoader\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.document_loaders import UnstructuredURLLoader\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.document_loaders import UnstructuredURLLoader\n",
            "/content/app.py:7: LangChainDeprecationWarning: Importing HuggingFaceEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.embeddings import HuggingFaceEmbeddings\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.embeddings import HuggingFaceEmbeddings\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.embeddings import HuggingFaceEmbeddings\n",
            "/content/app.py:8: LangChainDeprecationWarning: Importing FAISS from langchain.vectorstores is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.vectorstores import FAISS\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.vectorstores import FAISS\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.vectorstores import FAISS\n",
            "/content/app.py:5: LangChainDeprecationWarning: Importing UnstructuredURLLoader from langchain.document_loaders is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.document_loaders import UnstructuredURLLoader\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.document_loaders import UnstructuredURLLoader\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.document_loaders import UnstructuredURLLoader\n",
            "/content/app.py:7: LangChainDeprecationWarning: Importing HuggingFaceEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.embeddings import HuggingFaceEmbeddings\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.embeddings import HuggingFaceEmbeddings\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.embeddings import HuggingFaceEmbeddings\n",
            "/content/app.py:8: LangChainDeprecationWarning: Importing FAISS from langchain.vectorstores is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.vectorstores import FAISS\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.vectorstores import FAISS\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.vectorstores import FAISS\n",
            "Error fetching or processing https://hackrx.blob.core.windows.net/assets/Arogya%20Sanjeevani%20Policy%20-%20CIN%20-%20U10200WB1906GOI001713%201.pdf?sv=2023-01-03&st=2025-07-21T08%3A29%3A02Z&se=2025-09-22T08%3A29%3A00Z&sr=b&sp=r&sig=nzrz1K9Iurt%2BBXom%2FB%2BMPTFMFP3PRnIvEsipAX10Ig4%3D, exception: partition_pdf() is not available because one or more dependencies are not installed. Use: pip install \"unstructured[pdf]\" (including quotes) to install the required dependencies\n",
            "/content/app.py:27: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
            "2025-08-09 18:23:40.872784: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1754763820.934185   36472 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1754763820.950805   36472 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1754763821.005137   36472 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754763821.005229   36472 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754763821.005236   36472 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754763821.005239   36472 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[31m‚îÄ‚îÄ\u001b[0m\u001b[31m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[31m‚îÄ‚îÄ\u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/streamlit/runtime/scriptrunner/\u001b[0m\u001b[1;33mexec_code.py\u001b[0m: \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[94m128\u001b[0m in \u001b[92mexec_func_with_error_handling\u001b[0m                                                 \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/streamlit/runtime/scriptrunner/\u001b[0m\u001b[1;33mscript_runner\u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[1;33m.py\u001b[0m:\u001b[94m669\u001b[0m in \u001b[92mcode_to_exec\u001b[0m                                                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/content/\u001b[0m\u001b[1;33mapp.py\u001b[0m:\u001b[94m64\u001b[0m in \u001b[92m<module>\u001b[0m                                                       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m61 \u001b[0m\u001b[94mif\u001b[0m st.button(\u001b[33m\"\u001b[0m\u001b[33mGet Answers\u001b[0m\u001b[33m\"\u001b[0m):                                                    \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m62 \u001b[0m\u001b[2m‚îÇ   \u001b[0m\u001b[94mif\u001b[0m document_url \u001b[95mand\u001b[0m questions_input.strip():                                \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m63 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0mquestions = [q.strip() \u001b[94mfor\u001b[0m q \u001b[95min\u001b[0m questions_input.split(\u001b[33m\"\u001b[0m\u001b[33m\\n\u001b[0m\u001b[33m\"\u001b[0m) \u001b[94mif\u001b[0m q.strip( \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m‚ù± \u001b[0m64 \u001b[2m‚îÇ   ‚îÇ   \u001b[0manswers = \u001b[1;4mprocess_query(document_url, questions)\u001b[0m                        \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m65 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0m\u001b[94mfor\u001b[0m q, a \u001b[95min\u001b[0m \u001b[96mzip\u001b[0m(questions, answers):                                    \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m66 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0mst.markdown(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m**Q:** \u001b[0m\u001b[33m{\u001b[0mq\u001b[33m}\u001b[0m\u001b[33m\\n\u001b[0m\u001b[33m\\n\u001b[0m\u001b[33m**A:** \u001b[0m\u001b[33m{\u001b[0ma\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m)                            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m67 \u001b[0m\u001b[2m‚îÇ   \u001b[0m\u001b[94melse\u001b[0m:                                                                       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/content/\u001b[0m\u001b[1;33mapp.py\u001b[0m:\u001b[94m28\u001b[0m in \u001b[92mprocess_query\u001b[0m                                                  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m25 \u001b[0m\u001b[2m‚îÇ   \u001b[0mchunks = splitter.split_documents(docs)                                     \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m26 \u001b[0m\u001b[2m‚îÇ   \u001b[0m                                                                            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m27 \u001b[0m\u001b[2m‚îÇ   \u001b[0membeddings = HuggingFaceEmbeddings(model_name=\u001b[33m\"\u001b[0m\u001b[33mall-MiniLM-L6-v2\u001b[0m\u001b[33m\"\u001b[0m)           \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m‚ù± \u001b[0m28 \u001b[2m‚îÇ   \u001b[0mvectorstore = \u001b[1;4mFAISS.from_documents(chunks, embeddings)\u001b[0m                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m29 \u001b[0m\u001b[2m‚îÇ   \u001b[0mretriever = vectorstore.as_retriever()                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m30 \u001b[0m\u001b[2m‚îÇ   \u001b[0m                                                                            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m31 \u001b[0m\u001b[2m‚îÇ   \u001b[0mllm = ChatGoogleGenerativeAI(model=\u001b[33m\"\u001b[0m\u001b[33mgemini-pro\u001b[0m\u001b[33m\"\u001b[0m, temperature=\u001b[94m0.1\u001b[0m)           \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/langchain_core/vectorstores/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m848\u001b[0m in   \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[92mfrom_documents\u001b[0m                                                                       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 845 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m\u001b[94mif\u001b[0m \u001b[96many\u001b[0m(ids):                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 846 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   \u001b[0mkwargs[\u001b[33m\"\u001b[0m\u001b[33mids\u001b[0m\u001b[33m\"\u001b[0m] = ids                                           \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 847 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0m                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m‚ù± \u001b[0m 848 \u001b[2m‚îÇ   ‚îÇ   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[1;4;96mcls\u001b[0m\u001b[1;4m.from_texts(texts, embedding, metadatas=metadatas, **kwargs\u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 849 \u001b[0m\u001b[2m‚îÇ   \u001b[0m                                                                          \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 850 \u001b[0m\u001b[2m‚îÇ   \u001b[0m\u001b[1;95m@classmethod\u001b[0m                                                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 851 \u001b[0m\u001b[2m‚îÇ   \u001b[0m\u001b[94masync\u001b[0m \u001b[94mdef\u001b[0m\u001b[90m \u001b[0m\u001b[92mafrom_documents\u001b[0m(                                                \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/langchain_community/vectorstores/\u001b[0m\u001b[1;33mfaiss.py\u001b[0m:\u001b[94m10\u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[94m44\u001b[0m in \u001b[92mfrom_texts\u001b[0m                                                                     \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1041 \u001b[0m\u001b[2;33m‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m\u001b[33mfaiss = FAISS.from_texts(texts, embeddings)\u001b[0m                   \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1042 \u001b[0m\u001b[2;33m‚îÇ   ‚îÇ   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                   \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1043 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0membeddings = embedding.embed_documents(texts)                         \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m‚ù± \u001b[0m1044 \u001b[2m‚îÇ   ‚îÇ   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mcls\u001b[0m.__from(                                                    \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1045 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0mtexts,                                                            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1046 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0membeddings,                                                       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1047 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0membedding,                                                        \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/langchain_community/vectorstores/\u001b[0m\u001b[1;33mfaiss.py\u001b[0m:\u001b[94m10\u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[94m01\u001b[0m in \u001b[92m__from\u001b[0m                                                                         \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 998 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0mindex = faiss.IndexFlatIP(\u001b[96mlen\u001b[0m(embeddings[\u001b[94m0\u001b[0m]))                     \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 999 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0m\u001b[94melse\u001b[0m:                                                                 \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1000 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m\u001b[2m# Default to L2, currently other metric types not initialized.\u001b[0m    \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m‚ù± \u001b[0m1001 \u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0mindex = faiss.IndexFlatL2(\u001b[96mlen\u001b[0m(\u001b[1;4membeddings[\u001b[0m\u001b[1;4;94m0\u001b[0m\u001b[1;4m]\u001b[0m))                     \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1002 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0mdocstore = kwargs.pop(\u001b[33m\"\u001b[0m\u001b[33mdocstore\u001b[0m\u001b[33m\"\u001b[0m, InMemoryDocstore())                 \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1003 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0mindex_to_docstore_id = kwargs.pop(\u001b[33m\"\u001b[0m\u001b[33mindex_to_docstore_id\u001b[0m\u001b[33m\"\u001b[0m, {})         \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1004 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0mvecstore = \u001b[96mcls\u001b[0m(                                                       \u001b[31m \u001b[0m\n",
            "\u001b[31m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\n",
            "\u001b[1;91mIndexError: \u001b[0mlist index out of range\n",
            "/content/app.py:5: LangChainDeprecationWarning: Importing UnstructuredURLLoader from langchain.document_loaders is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.document_loaders import UnstructuredURLLoader\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.document_loaders import UnstructuredURLLoader\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.document_loaders import UnstructuredURLLoader\n",
            "/content/app.py:7: LangChainDeprecationWarning: Importing HuggingFaceEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.embeddings import HuggingFaceEmbeddings\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.embeddings import HuggingFaceEmbeddings\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.embeddings import HuggingFaceEmbeddings\n",
            "/content/app.py:8: LangChainDeprecationWarning: Importing FAISS from langchain.vectorstores is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.vectorstores import FAISS\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.vectorstores import FAISS\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.vectorstores import FAISS\n",
            "/content/app.py:5: LangChainDeprecationWarning: Importing UnstructuredURLLoader from langchain.document_loaders is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.document_loaders import UnstructuredURLLoader\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.document_loaders import UnstructuredURLLoader\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.document_loaders import UnstructuredURLLoader\n",
            "/content/app.py:7: LangChainDeprecationWarning: Importing HuggingFaceEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.embeddings import HuggingFaceEmbeddings\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.embeddings import HuggingFaceEmbeddings\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.embeddings import HuggingFaceEmbeddings\n",
            "/content/app.py:8: LangChainDeprecationWarning: Importing FAISS from langchain.vectorstores is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.vectorstores import FAISS\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.vectorstores import FAISS\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.vectorstores import FAISS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pkill -f ngrok  # Kill all ngrok processes in Colab\n"
      ],
      "metadata": {
        "id": "79Z5P3v79Z36"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Kill old tunnels so we don't get ngrok ERR_108\n",
        "ngrok.kill()\n",
        "\n",
        "# Open tunnel on Streamlit's default port 8501\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"‚úÖ Public Streamlit URL:\", public_url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGCGe4-X6PRw",
        "outputId": "03fbc785-466b-4a57-aaa8-456b54ca9037"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Public Streamlit URL: NgrokTunnel: \"https://b727be46d11e.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "\n",
        "st.set_page_config(page_title=\"Bajaj Policy Q&A\", layout=\"wide\")\n",
        "\n",
        "st.title(\"üìÑ Bajaj Policy Q&A\")\n",
        "st.write(\"Ask questions about a document (PDF/URL) using Gemini-Pro\")\n",
        "\n",
        "# User input for document URL\n",
        "doc_url = st.text_input(\"Document URL (PDF or web page)\", \"\")\n",
        "\n",
        "# User input for questions\n",
        "questions_text = st.text_area(\"Questions (one per line)\", \"\")\n",
        "\n",
        "# Button to process\n",
        "if st.button(\"Get Answers\"):\n",
        "    if doc_url and questions_text.strip():\n",
        "        questions = [q.strip() for q in questions_text.split(\"\\n\") if q.strip()]\n",
        "        with st.spinner(\"Processing your queries...\"):\n",
        "            answers = process_query(doc_url, questions)\n",
        "        st.subheader(\"Answers\")\n",
        "        for q, a in zip(questions, answers):\n",
        "            st.markdown(f\"**Q:** {q}\\n\\n**A:** {a}\\n---\")\n",
        "    else:\n",
        "        st.error(\"‚ö† Please provide both a document URL and at least one question.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zJphse-7Ajw",
        "outputId": "ae296407-7da5-48d0-8ae0-7ec018db17eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-08-09 17:51:35.355 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-09 17:51:35.356 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-09 17:51:35.500 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n",
            "2025-08-09 17:51:35.501 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-09 17:51:35.502 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-09 17:51:35.504 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-09 17:51:35.504 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-09 17:51:35.505 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-09 17:51:35.506 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-09 17:51:35.508 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-09 17:51:35.509 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-09 17:51:35.510 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-09 17:51:35.510 Session state does not function when running a script without `streamlit run`\n",
            "2025-08-09 17:51:35.511 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-09 17:51:35.512 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-09 17:51:35.513 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-09 17:51:35.514 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-09 17:51:35.515 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-09 17:51:35.515 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-09 17:51:35.516 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-09 17:51:35.517 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-09 17:51:35.517 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-09 17:51:35.518 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-09 17:51:35.520 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-09 17:51:35.521 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-09 17:51:35.522 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-09 17:51:35.522 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-09 17:51:35.523 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-09 17:51:35.524 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run your_file.py & sleep 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtkJSJBk7DdC",
        "outputId": "386820b7-a078-48d0-ead4-56efaf863751"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: streamlit run [OPTIONS] TARGET [ARGS]...\n",
            "Try 'streamlit run --help' for help.\n",
            "\n",
            "Error: Invalid value: File does not exist: your_file.py\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}